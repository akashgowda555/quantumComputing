{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "author=\"akash\"\n",
    "date = 'Aug 18,2023'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset can be downloaded from https://www.unb.ca/cic/datasets/ids-2017.html or you can directly below shell\n",
    "You need to just unzip and put it same directory with this CNN.ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset directly and unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown --id 1-t3RdDpmqMs4ABt9oobSapeNYTZJ9tpu\n",
    "# !unzip MachineLearningCSV.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "%load_ext autoreload\n",
    "\n",
    "import qiskit\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from qiskit import execute\n",
    "from qiskit.circuit import Parameter,ControlledGate\n",
    "from qiskit import Aer\n",
    "import random\n",
    "import time\n",
    "from torchsummary import summary\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "\n",
    "from qiskit import *\n",
    "provider = qiskit\n",
    "backend = Aer.get_backend(\"aer_simulator\", device=\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = './MachineLearningCVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataroot,file_ending='*.pcap_ISCX.csv'):\n",
    "    if file_ending==None:\n",
    "        print(\"please specify file ending pattern for glob\")\n",
    "        exit()\n",
    "    print(join(dataroot,file_ending))\n",
    "    filenames = [i for i in glob.glob(join(dataroot,file_ending))]\n",
    "    print(filenames)\n",
    "    combined_csv = pd.concat([pd.read_csv(f,dtype=object) for f in filenames],sort=False)\n",
    "    print(combined_csv.shape)\n",
    "    return combined_csv\n",
    "\n",
    "def make_value2index(attacks):\n",
    "    #make dictionary\n",
    "    attacks = sorted(attacks)\n",
    "    d = {}\n",
    "    counter=0\n",
    "    for attack in attacks:\n",
    "        d[attack] = counter\n",
    "        counter+=1\n",
    "    return d\n",
    "\n",
    "def encode_label(Y_str):\n",
    "    labels_d = make_value2index(np.unique(Y_str))\n",
    "    Y = [labels_d[y_str] for y_str  in Y_str]\n",
    "    Y = np.array(Y)\n",
    "    return np.array(Y)\n",
    "\n",
    "# reads csv file and returns np array of X,y -> of shape (N,D) and (N,1)\n",
    "def load_data(dataroot):\n",
    "    data = read_data(dataroot,'*.csv')\n",
    "    num_records,num_features = data.shape \n",
    "    print(\"There are {} flow records with {} feature dimension\".format(num_records,num_features))\n",
    "    print('Data loaded.\\nData preprocessing started...')\n",
    "    # there is white spaces in columns names e.g. ' Destination Port'\n",
    "    # So strip the whitespace from  column names\n",
    "    data = data.rename(columns=lambda x: x.strip())\n",
    "    print('Stripped column names with whitespaces')\n",
    "\n",
    "    df_label = data['Label']\n",
    "    data = data.drop(columns=['Flow Packets/s','Flow Bytes/s','Label'])\n",
    "    print('remove unnecessary columns: ',['Flow Packets/s','Flow Bytes/s','Label'])\n",
    "    \n",
    "    nan_count = data.isnull().sum().sum()\n",
    "    print('There are {} nan entries'.format(nan_count))\n",
    "    \n",
    "    if nan_count>0:\n",
    "        data.fillna(data.mean(), inplace=True)\n",
    "        print('Filled NAN')\n",
    "\n",
    "    data = data.astype(float).apply(pd.to_numeric)\n",
    "    print('Converted to numeric')\n",
    "    \n",
    "    # lets count if there is NaN values in our dataframe( AKA missing features)\n",
    "    assert data.isnull().sum().sum()==0, \"There should not be any NaN\"\n",
    "    X = data.values\n",
    "    print(X.shape)\n",
    "    y = encode_label(df_label.values)\n",
    "    return (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MachineLearningCVE/*.csv\n",
      "['./MachineLearningCVE/output_file.csv']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m X,y \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataroot\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# reads csv file and returns np array of X,y -> of shape (N,D) and (N,1)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 30\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(dataroot)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(dataroot):\n\u001b[0;32m---> 30\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataroot\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     num_records,num_features \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape \n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m flow records with \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m feature dimension\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(num_records,num_features))\n",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m, in \u001b[0;36mread_data\u001b[0;34m(dataroot, file_ending)\u001b[0m\n\u001b[1;32m      6\u001b[0m filenames \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(join(dataroot,file_ending))]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(filenames)\n\u001b[0;32m----> 8\u001b[0m combined_csv \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mread_csv(f,dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m (\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m)],sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(combined_csv\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m combined_csv\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "X,y = load_data(dataroot) # reads csv file and returns np array of X,y -> of shape (N,D) and (N,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imbalance\n",
    "1. It is curucial to adress this issue in order to get decent performance\n",
    "2. It also affects evaluation, we should calculate  `balanced accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "def normalize(data):\n",
    "    data = data.astype(np.float32)\n",
    "    eps = 1e-15\n",
    "\n",
    "    mask = data==-1\n",
    "    data[mask]=0\n",
    "    mean_i = np.mean(data,axis=0)\n",
    "    min_i = np.min(data,axis=0) #  to leave -1 (missing features) values as is and exclude in normilizing\n",
    "    max_i = np.max(data,axis=0)\n",
    "\n",
    "    r = max_i-min_i+eps\n",
    "    data = (data-mean_i)/r  # zero centered \n",
    "\n",
    "    #deal with missing features -1\n",
    "    data[mask] = 0        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X and y dataset shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9999, 76)\n",
      "(9999,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000', '0001', '0010', '0011', '0100', '0101', '0110', '0111', '1000', '1001', '1010', '1011', '1100', '1101', '1110', '1111']\n"
     ]
    }
   ],
   "source": [
    "NUM_QUBITS = 4\n",
    "NUM_SHOTS = 1024\n",
    "from qiskit import *\n",
    "import qiskit\n",
    "\n",
    "import itertools\n",
    "def create_QC_OUTPUTS():\n",
    "    measurements = list(itertools.product([0, 1], repeat=NUM_QUBITS))\n",
    "    return [''.join([str(bit) for bit in measurement]) for measurement in measurements]\n",
    "\n",
    "QC_OUTPUTS = create_QC_OUTPUTS()\n",
    "print(QC_OUTPUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCNeuron():\n",
    "    def __init__(self, n_qubits, backend, shots):\n",
    "        self.qc = qiskit.QuantumCircuit(n_qubits)\n",
    "        self.n_qubits = n_qubits\n",
    "        self.thetas = {k : Parameter('Theta'+str(k))for k in range(2*self.n_qubits)}\n",
    "\n",
    "        all_qubits = [i for i in range(n_qubits)]\n",
    "        self.qc.h(all_qubits)\n",
    "        self.qc.barrier()\n",
    "\n",
    "        for k in range(0, 4):\n",
    "          self.qc.ry(self.thetas[k], k)\n",
    "\n",
    "        self.qc.barrier()\n",
    "\n",
    "        self.qc.cx(1, 0)\n",
    "        self.qc.cx(2, 0)\n",
    "        self.qc.cx(3, 0)\n",
    "        self.qc.cx(2, 1)\n",
    "        self.qc.cx(3, 1)\n",
    "        self.qc.cx(3, 2)\n",
    "        self.qc.barrier()\n",
    "\n",
    "        for k in range(0, 4):\n",
    "          self.qc.ry(self.thetas[k+4], k)\n",
    "\n",
    "        self.qc.measure_all()\n",
    "        # ---------------------------\n",
    "\n",
    "        self.backend = backend\n",
    "        self.shots = shots\n",
    "\n",
    "        self.tqc = transpile(self.qc, backend=self.backend)\n",
    "\n",
    "\n",
    "    def N_qubit_expectation_Z(self,counts, shots, nr_qubits):\n",
    "        expects = np.zeros(len(QC_OUTPUTS))\n",
    "        for k in range(len(QC_OUTPUTS)):\n",
    "            key = QC_OUTPUTS[k]\n",
    "            perc = counts.get(key, 0)/shots\n",
    "            expects[k] = perc\n",
    "        return expects\n",
    "\n",
    "    def run(self, i):\n",
    "        params = i.cpu().data.numpy()\n",
    "        parameters = {}\n",
    "\n",
    "        for k in range(2*self.n_qubits):\n",
    "          parameters[self.thetas[k]] = params[k].item()\n",
    "\n",
    "        qobj = assemble(self.tqc,\n",
    "                        shots=self.shots,\n",
    "                        parameter_binds = [parameters])\n",
    "\n",
    "        job = self.backend.run(qobj)\n",
    "        res =  self.N_qubit_expectation_Z(job.result().get_counts(), self.shots, self.n_qubits)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">        ┌───┐ ░ ┌────────────┐ ░ ┌───┐┌───┐┌───┐                ░ ┌────────────┐ ░ ┌─┐         \n",
       "   q_0: ┤ H ├─░─┤ Ry(Theta0) ├─░─┤ X ├┤ X ├┤ X ├────────────────░─┤ Ry(Theta4) ├─░─┤M├─────────\n",
       "        ├───┤ ░ ├────────────┤ ░ └─┬─┘└─┬─┘└─┬─┘┌───┐┌───┐      ░ ├────────────┤ ░ └╥┘┌─┐      \n",
       "   q_1: ┤ H ├─░─┤ Ry(Theta1) ├─░───■────┼────┼──┤ X ├┤ X ├──────░─┤ Ry(Theta5) ├─░──╫─┤M├──────\n",
       "        ├───┤ ░ ├────────────┤ ░        │    │  └─┬─┘└─┬─┘┌───┐ ░ ├────────────┤ ░  ║ └╥┘┌─┐   \n",
       "   q_2: ┤ H ├─░─┤ Ry(Theta2) ├─░────────■────┼────■────┼──┤ X ├─░─┤ Ry(Theta6) ├─░──╫──╫─┤M├───\n",
       "        ├───┤ ░ ├────────────┤ ░             │         │  └─┬─┘ ░ ├────────────┤ ░  ║  ║ └╥┘┌─┐\n",
       "   q_3: ┤ H ├─░─┤ Ry(Theta3) ├─░─────────────■─────────■────■───░─┤ Ry(Theta7) ├─░──╫──╫──╫─┤M├\n",
       "        └───┘ ░ └────────────┘ ░                                ░ └────────────┘ ░  ║  ║  ║ └╥┘\n",
       "meas: 4/════════════════════════════════════════════════════════════════════════════╩══╩══╩══╩═\n",
       "                                                                                    0  1  2  3 </pre>"
      ],
      "text/plain": [
       "        ┌───┐ ░ ┌────────────┐ ░ ┌───┐┌───┐┌───┐                ░ ┌────────────┐ ░ ┌─┐         \n",
       "   q_0: ┤ H ├─░─┤ Ry(Theta0) ├─░─┤ X ├┤ X ├┤ X ├────────────────░─┤ Ry(Theta4) ├─░─┤M├─────────\n",
       "        ├───┤ ░ ├────────────┤ ░ └─┬─┘└─┬─┘└─┬─┘┌───┐┌───┐      ░ ├────────────┤ ░ └╥┘┌─┐      \n",
       "   q_1: ┤ H ├─░─┤ Ry(Theta1) ├─░───■────┼────┼──┤ X ├┤ X ├──────░─┤ Ry(Theta5) ├─░──╫─┤M├──────\n",
       "        ├───┤ ░ ├────────────┤ ░        │    │  └─┬─┘└─┬─┘┌───┐ ░ ├────────────┤ ░  ║ └╥┘┌─┐   \n",
       "   q_2: ┤ H ├─░─┤ Ry(Theta2) ├─░────────■────┼────■────┼──┤ X ├─░─┤ Ry(Theta6) ├─░──╫──╫─┤M├───\n",
       "        ├───┤ ░ ├────────────┤ ░             │         │  └─┬─┘ ░ ├────────────┤ ░  ║  ║ └╥┘┌─┐\n",
       "   q_3: ┤ H ├─░─┤ Ry(Theta3) ├─░─────────────■─────────■────■───░─┤ Ry(Theta7) ├─░──╫──╫──╫─┤M├\n",
       "        └───┘ ░ └────────────┘ ░                                ░ └────────────┘ ░  ║  ║  ║ └╥┘\n",
       "meas: 4/════════════════════════════════════════════════════════════════════════════╩══╩══╩══╩═\n",
       "                                                                                    0  1  2  3 "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circuit = QCNeuron(NUM_QUBITS, Aer.get_backend('aer_simulator', device=\"GPU\"), NUM_SHOTS)\n",
    "circuit.qc.draw(fold=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        if not hasattr(ctx, 'qc'):\n",
    "            ctx.qc = QCNeuron(NUM_QUBITS, SIMULATOR, shots=NUM_SHOTS)\n",
    "        exp_value = ctx.qc.run(i)\n",
    "        result = torch.tensor([exp_value]).to(device)\n",
    "        ctx.save_for_backward(result, i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        forward_tensor, i = ctx.saved_tensors\n",
    "        input_numbers = i\n",
    "        gradients = torch.Tensor()\n",
    "\n",
    "        for k in range(2*NUM_QUBITS):\n",
    "            shift_right = input_numbers.detach().clone()\n",
    "            shift_right[k] = shift_right[k] + SHIFT\n",
    "            shift_left = input_numbers.detach().clone()\n",
    "            shift_left[k] = shift_left[k] - SHIFT\n",
    "\n",
    "            expectation_right = ctx.qc.run(shift_right)\n",
    "            expectation_left  = ctx.qc.run(shift_left)\n",
    "\n",
    "            gradient = torch.tensor([expectation_right]) - torch.tensor([expectation_left])\n",
    "            gradients = torch.cat((gradients, gradient.float()))\n",
    "\n",
    "        result = torch.Tensor(gradients).to(device)\n",
    "\n",
    "        return (result.float() * grad_output.float()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 19:53:09.048490: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "from enum import Enum\n",
    "import logging\n",
    "from os.path import join\n",
    "\n",
    "class CNN5(nn.Module):\n",
    "    \"\"\" \n",
    "    5-layer Convolutional Neural Network Model\n",
    "    \"\"\"\n",
    "    def __init__(self,input_dim,num_classes,device):\n",
    "        super(CNN5, self).__init__()\n",
    "        # kernel\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        conv_layers = []\n",
    "        conv_layers.append(nn.Conv1d(in_channels=1,out_channels=64,kernel_size=3,padding=1)) # ;input_dim,64\n",
    "        conv_layers.append(nn.BatchNorm1d(64))\n",
    "        conv_layers.append(nn.ReLU(True))\n",
    "\n",
    "        conv_layers.append(nn.Conv1d(in_channels=64,out_channels=128,kernel_size=3,padding=1)) #(input_dim,128)\n",
    "        conv_layers.append(nn.BatchNorm1d(128))\n",
    "        conv_layers.append(nn.ReLU(True))\n",
    "\n",
    "        conv_layers.append(nn.Conv1d(in_channels=128,out_channels=256,kernel_size=3,padding=1)) #(input_dim,128)\n",
    "        conv_layers.append(nn.BatchNorm1d(256))\n",
    "        conv_layers.append(nn.ReLU(True))\n",
    "\n",
    "        conv_layers.append(nn.Conv1d(in_channels=256,out_channels=256,kernel_size=3,padding=1)) #(input_dim,128)\n",
    "        conv_layers.append(nn.BatchNorm1d(256))\n",
    "        conv_layers.append(nn.ReLU(True))\n",
    "\n",
    "        conv_layers.append(nn.Conv1d(in_channels=256,out_channels=128,kernel_size=3,padding=1)) #(input_dim,128)\n",
    "        conv_layers.append(nn.BatchNorm1d(128))\n",
    "        conv_layers.append(nn.ReLU(True))\n",
    "        \n",
    "        self.conv = nn.Sequential(*conv_layers).to(device)\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim*128, 2)\n",
    "#         self.qc = QuantumLayer.apply\n",
    "#         self.fc2 = nn.Linear(16, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, D = x.shape\n",
    "        x = x.view(batch_size,1,D)\n",
    "\n",
    "        x = self.conv(x)\n",
    "        # x = torch.flatten(x,1)\n",
    "        x = x.view(-1, input_dim*128)\n",
    "        x = self.fc1(x)\n",
    "#         x = np.pi*torch.tanh(x)\n",
    "\n",
    "        # Quantum circuit\n",
    "#         x = self.qc(x[0])\n",
    "#         x = F.relu(x)\n",
    "#         ##\n",
    "#         x = self.fc2(x.float())\n",
    "        x = F.softmax(x, 1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Method(Enum):\n",
    "    CNN5 = \"cnn5\"\n",
    "\n",
    "class Classifier:\n",
    "    def __init__(self,method,input_dim,num_classes,num_epochs,batch_size=100,lr=1e-3,reg=1e-5,runs_dir=None,seed=10):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = lr\n",
    "        self.reg= reg\n",
    "        self.runs_dir = runs_dir\n",
    "        self.seed = seed\n",
    "        #self.device = 'cuda'\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        file_handler = logging.FileHandler('training.log')\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "        file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(file_formatter)\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.info(\"Classifier initialized with method %s, input_dim %d, num_classes %d, num_epochs %d, batch_size %d, lr %f, reg %f, runs_dir %s\" % (method,input_dim,num_classes,num_epochs,batch_size,lr,reg,runs_dir))\n",
    "        \n",
    "        #self.model = nn.Linear(self.input_size, self.num_classes).to(self.device)       \n",
    "        if method==Method.CNN5.value:\n",
    "            self.device = torch.device('cpu')\n",
    "            self.model = CNN5(input_dim,num_classes=num_classes,device=self.device)    \n",
    "        else:\n",
    "            raise ValueError(\"Method must be one of 'softmax', 'cnn2', 'cnn5', 'nn3', or 'nn5'.\")\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=self.learning_rate,betas=(0.9,0.99),eps=1e-08, weight_decay=self.reg, amsgrad=False)\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        self.logger.info('Starting training process...')\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=self.seed)\n",
    "        for dev_index, val_index in sss.split(X, Y): # runs only once\n",
    "                X_dev = X[dev_index]\n",
    "                Y_dev = Y[dev_index]\n",
    "                X_val = X[val_index]\n",
    "                Y_val = Y[val_index]  \n",
    "        \n",
    "        writer = SummaryWriter(self.runs_dir) \n",
    "\n",
    "        tensor_x = torch.stack([torch.Tensor(i) for i in X_dev]).to(self.device)\n",
    "        tensor_y = torch.LongTensor(Y_dev).to(self.device) # checked working correctly\n",
    "\n",
    "        dataset = utils.TensorDataset(tensor_x,tensor_y)        \n",
    "        train_loader = utils.DataLoader(dataset,batch_size=self.batch_size) \n",
    "        N = tensor_x.shape[0]\n",
    "\n",
    "        num_epochs = self.num_epochs\n",
    "\n",
    "        model  = self.model\n",
    "        best_acc = None\n",
    "        best_epoch = None\n",
    "\n",
    "        filepath = join(self.runs_dir,'checkpoint.pth')\n",
    "        if os.path.isfile(filepath):\n",
    "            checkpoint = self.load_checkpoint(filepath)\n",
    "            best_epoch = checkpoint['epoch']\n",
    "            best_batch = checkpoint['batch']\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            pred = self.predict(X_val)\n",
    "            best_acc = metrics.balanced_accuracy_score(Y_val,pred)*100\n",
    "            resume_epoch = best_epoch+1  \n",
    "            resume_batch = best_batch+1\n",
    "        else:\n",
    "            resume_epoch = 0\n",
    "            resume_batch = 0\n",
    "            best_acc = -1\n",
    "            best_epoch = 0\n",
    "\n",
    "        no_improvement = 0\n",
    "        for epoch in range(resume_epoch,num_epochs):\n",
    "            for i,(xi,yi) in enumerate(train_loader):\n",
    "                if epoch==resume_epoch and i<resume_batch:\n",
    "                    continue\n",
    "                    \n",
    "                outputs = model(xi)\n",
    "                loss = self.criterion(outputs,yi)\n",
    "\n",
    "                loss.requires_grad\n",
    "                seen_so_far = self.batch_size*(epoch*len(train_loader)+i+1) # fixes issues with different batch size\n",
    "                writer.add_scalar('Loss/train',loss.item(),seen_so_far)\n",
    "                #batckward, optimize\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if (seen_so_far/self.batch_size)%50==0:\n",
    "                    pred = self.predict(X_val)\n",
    "                    balanced_acc = metrics.balanced_accuracy_score(Y_val,pred)*100\n",
    "                    if balanced_acc > best_acc:\n",
    "                        best_acc = balanced_acc\n",
    "                        best_epoch = epoch\n",
    "                        checkpoint = {\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer' : self.optimizer.state_dict(),\n",
    "                        'epoch':epoch,\n",
    "                        'batch': i,\n",
    "                        'batch_size': self.batch_size\n",
    "                        }\n",
    "                        self.save(checkpoint)\n",
    "                        no_improvement =0\n",
    "                    else:\n",
    "                        no_improvement+=1\n",
    "                        if no_improvement>=10:\n",
    "                            self.logger.warning(\"No improvement in accuracy for 10 iterations.\")\n",
    "                            return\n",
    "                    self.logger.debug('Epoch [%d/%d], Step [%d/%d], Loss: %.4f', epoch+1, num_epochs, i+1, len(Y_dev)//self.batch_size, loss.item())\n",
    "\n",
    "\n",
    "                    writer.add_scalar('Accuracy/Balanced Val',balanced_acc,seen_so_far)\n",
    "\n",
    "                    acc = metrics.accuracy_score(Y_val,pred)*100\n",
    "                    writer.add_scalar('Accuracy/Val',acc,seen_so_far)\n",
    "        writer.close()\n",
    "\n",
    "    def predict(self,x,eval_mode=False):\n",
    "        tensor_x = torch.stack([torch.Tensor(i) for i in x]).to(self.device)\n",
    "        bs = self.batch_size\n",
    "        num_batch = x.shape[0]//bs +1*(x.shape[0]%bs!=0)\n",
    "\n",
    "        pred = torch.zeros(0,dtype=torch.int64).to(self.device)\n",
    "        \n",
    "        if eval_mode:\n",
    "            model = self.load_model()\n",
    "        else:\n",
    "            model = self.model\n",
    "        model.eval()        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(num_batch):\n",
    "                xi = tensor_x[i*bs:(i+1)*bs]\n",
    "                outputs = model(xi)\n",
    "                _, predicted_labels = torch.max(outputs.data,1)\n",
    "                pred = torch.cat((pred,predicted_labels))\n",
    "\n",
    "        return pred.cpu().numpy()\n",
    "\n",
    "\n",
    "    def save(self,checkpoint):\n",
    "        path = join(self.runs_dir,'checkpoint.pth')\n",
    "        torch.save(checkpoint,path)\n",
    "\n",
    "    \n",
    "    def load_checkpoint(self,filepath):\n",
    "        if os.path.isfile(filepath):\n",
    "            checkpoint = torch.load(filepath)\n",
    "            print(\"Loaded {} model trained with batch_size = {}, seen {} epochs and {} mini batches\".\n",
    "                format(self.runs_dir,checkpoint['batch_size'],checkpoint['epoch'],checkpoint['batch'])) \n",
    "            return checkpoint\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "            \n",
    "    def load_model(self,inference_mode=True):\n",
    "        filepath = join(self.runs_dir,'checkpoint.pth')\n",
    "        checkpoint = self.load_checkpoint(filepath)\n",
    "        \n",
    "        model = self.model\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        \n",
    "        if inference_mode:\n",
    "            for parameter in model.parameters():\n",
    "                parameter.requires_grad = False\n",
    "            model.eval()\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %autoreload 2\n",
    "\n",
    "def ensure_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "def getClassifier(args,runs_dir=None):\n",
    "    (method,optim,lr,reg,batch_size,input_dim,num_class,num_epochs) = args\n",
    "    if runs_dir is not None:\n",
    "        ensure_dir(runs_dir)\n",
    "    \n",
    "    clf = Classifier(method,input_dim,num_class,lr=lr,reg=reg,num_epochs=num_epochs,\n",
    "                        batch_size=batch_size,runs_dir=runs_dir)\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Fold #0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "K=5\n",
    "skf = StratifiedKFold(n_splits=K,random_state=SEED, shuffle=True)\n",
    "for fold_index, (train_index,test_index) in enumerate(skf.split(X,y)):# runs only once \n",
    "        print('---------------------------------------------')\n",
    "        print('Fold #{}'.format(fold_index))    \n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.set_num_threads(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We balance data as follows:\n",
    "#1) oversample small classes so that their population/count is equal to mean_number_of_samples_per_class\n",
    "#2) undersample large classes so that their count is equal to mean_number_of_samples_per_class\n",
    "def balance_data(X,y,seed):\n",
    "    np.random.seed(seed)\n",
    "    unique,counts = np.unique(y,return_counts=True)\n",
    "    mean_samples_per_class = int(round(np.mean(counts)))\n",
    "    N,D = X.shape #(number of examples, number of features)\n",
    "    new_X = np.empty((0,D)) \n",
    "    new_y = np.empty((0),dtype=int)\n",
    "    for i,c in enumerate(unique):\n",
    "        temp_x = X[y==c]\n",
    "        indices = np.random.choice(temp_x.shape[0],mean_samples_per_class) # gets `mean_samples_per_class` indices of class `c`\n",
    "        new_X = np.concatenate((new_X,temp_x[indices]),axis=0) # now we put new data into new_X \n",
    "        temp_y = np.ones(mean_samples_per_class,dtype=int)*c\n",
    "        new_y = np.concatenate((new_y,temp_y),axis=0)\n",
    "        \n",
    "    # in order to break class order in data we need shuffling\n",
    "    indices = np.arange(new_y.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    new_X =  new_X[indices,:]\n",
    "    new_y = new_y[indices]\n",
    "    return (new_X,new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_SHOTS = 750\n",
    "SHIFT = np.pi/4\n",
    "\n",
    "SIMULATOR = Aer.get_backend('aer_simulator')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.001\n",
      "balancing data ...\n",
      "Fit data to model.\n",
      "Loaded ./MachineLearningCVE/runs/1D-CNN_conv_5_fc_1/5th_run/optim_Adam_lr_0.001_reg_0.001_bs_1 model trained with batch_size = 1, seen 0 epochs and 49 mini batches\n",
      "Try prediction .. \n",
      "Model is trained in 47.533984899520874 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#hyper-params\n",
    "batch_size = 1 #5*1024 # increasing batch size with more gpu added\n",
    "optim = 'Adam'\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "num_class = len(np.unique(y_train))\n",
    "num_epochs = 1\n",
    "learning_rates = [1e-3]\n",
    "regularizations = [1e-3]\n",
    "\n",
    "\n",
    "accuracies = {}\n",
    "best_model = None\n",
    "best_acc = -1\n",
    "architecture = '1D-CNN_conv_5_fc_1'\n",
    "run_number = 5\n",
    "method='cnn5'\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in regularizations:\n",
    "        print (reg, lr)\n",
    "        classifier_args = (method,optim,lr,reg,batch_size,input_dim,num_class,num_epochs)\n",
    "        config =  '{}/{}th_run/optim_{}_lr_{}_reg_{}_bs_{}'.format(architecture,run_number,optim,lr,reg,batch_size)\n",
    "        runs_dir = join(dataroot,'runs',config)\n",
    "        \n",
    "        X_train = X_train.astype(float)\n",
    "        y_train = y_train.astype(int)\n",
    "        p = np.random.permutation(len(y_train))\n",
    "        X_train = X_train[p]\n",
    "        y_train = y_train[p]\n",
    "        print ('balancing data ...')\n",
    "        X_train,y_train = balance_data(X_train,y_train,seed=SEED)\n",
    "        \n",
    "        tick = time.time()\n",
    "        clf = getClassifier(classifier_args,runs_dir)\n",
    "        print ('Fit data to model.')\n",
    "\n",
    "        clf.fit(X_train,y_train)\n",
    "        pred = clf.predict(X_test,eval_mode=True)\n",
    "        \n",
    "        print ('Try prediction .. ')\n",
    "        acc = metrics.balanced_accuracy_score(y_test,pred)\n",
    "        if acc >best_acc:\n",
    "            best_model = clf\n",
    "            best_acc = acc\n",
    "        accuracies[(lr,reg)]=acc\n",
    "        tock = time.time()\n",
    "        print(\"Model is trained in {} sec\".format(tock-tick))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.001, 0.001)\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# accuracies for CV\n",
    "for x in accuracies:\n",
    "    print(x)\n",
    "    print(accuracies[x])\n",
    "results = accuracies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cross-validation results\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "\n",
    "# plot validation accuracy\n",
    "marker_size=100\n",
    "colors = [results[x] for x in results] # default size of markers is 20\n",
    "\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors, cmap=plt.cm.coolwarm)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('Net intrusion(CIC-IDS-2017) validation accuracy')\n",
    "plt.savefig('./method_{}_arch_run_{}-2.png'.format(method,architecture,run_number))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(clf.model, (76,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
